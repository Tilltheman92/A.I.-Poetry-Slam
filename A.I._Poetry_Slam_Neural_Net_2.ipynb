{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A.I._Poetry_Slam_Neural_Net_2.ipynb","provenance":[{"file_id":"1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK","timestamp":1594115474285}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Egeile5Kc7t3","colab_type":"text"},"source":["Using textgenrnn on tensorflow backend to generate some poetry. This model can be used letter- or word-based. I used it word-based.\n","I found this model on a youtube video by Max Woolf\n","https://www.youtube.com/watch?v=RW7mP6BfZuY&list=PLEVLQU6Zt5WCI19mIj3Cmg0sWmZ48Gr5b&index=4"]},{"cell_type":"code","metadata":{"id":"qbs57FeUybgL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594199130821,"user_tz":-120,"elapsed":493,"user":{"displayName":"Tilman Fabricius","photoUrl":"","userId":"16612614316801778267"}},"outputId":"870a9622-fedb-4186-dca8-2c32f9a82295"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QVyp4Mm6hm0d","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBkpRgBCBS2_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1594199142870,"user_tz":-120,"elapsed":10332,"user":{"displayName":"Tilman Fabricius","photoUrl":"","userId":"16612614316801778267"}},"outputId":"1c603a98-0eef-49ec-a5b4-ff5546ffa733"},"source":["!pip install -q textgenrnn\n","from google.colab import files\n","from textgenrnn import textgenrnn\n","from datetime import datetime\n","import os"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NLB3tqPxdKqs","colab_type":"text"},"source":["Setting the model according to recomendations of author"]},{"cell_type":"code","metadata":{"id":"P8wSlgXoDPCR","colab_type":"code","colab":{}},"source":["model_cfg = {\n","    'word_level': True,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n","    'rnn_size': 128,   # number of LSTM cells of each layer (128/256 recommended)\n","    'rnn_layers': 5,   # number of LSTM layers (>=2 recommended)\n","    'rnn_bidirectional': True,   # consider text both forwards and backward, can give a training boost\n","    'max_length': 8,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n","    'max_words': 10000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n","}\n","\n","train_cfg = {\n","    'line_delimited': False,   # set to True if each text has its own line in the source file\n","    'num_epochs': 30,   # set higher to train the model for longer\n","    'gen_epochs': 50,   # generates sample text from model after given number of epochs\n","    'train_size': 0.8,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n","    'dropout': 0.0,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n","    'validation': False,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n","    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gacdMHucdkeI","colab_type":"text"},"source":["Importing the data"]},{"cell_type":"code","metadata":{"id":"6OFnPCLADfll","colab_type":"code","colab":{}},"source":["file_name = \"Goethe_all.txt\"\n","model_name = 'Goethe_all'   # change to set file name of resulting trained models/texts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4z-dFuuCdnb1","colab_type":"text"},"source":["Training the model"]},{"cell_type":"code","metadata":{"id":"aeXshJM-Cuaf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594201577445,"user_tz":-120,"elapsed":2382704,"user":{"displayName":"Tilman Fabricius","photoUrl":"","userId":"16612614316801778267"}},"outputId":"b2bad7d2-5680-409f-c30b-87ae3aeed78c"},"source":["textgen = textgenrnn(name=model_name)\n","\n","train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n","\n","train_function(\n","    file_path=file_name,\n","    new_model=True,\n","    num_epochs=train_cfg['num_epochs'],\n","    gen_epochs=train_cfg['gen_epochs'],\n","    batch_size=1024,\n","    train_size=train_cfg['train_size'],\n","    dropout=train_cfg['dropout'],\n","    validation=train_cfg['validation'],\n","    is_csv=train_cfg['is_csv'],\n","    rnn_layers=model_cfg['rnn_layers'],\n","    rnn_size=model_cfg['rnn_size'],\n","    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n","    max_length=model_cfg['max_length'],\n","    dim_embeddings=100,\n","    word_level=model_cfg['word_level'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training new model w/ 5-layer, 128-cell Bidirectional LSTMs\n","Training on 469,618 word sequences.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Epoch 1/30\n","458/458 [==============================] - 86s 188ms/step - loss: 6.0609\n","Epoch 2/30\n","458/458 [==============================] - 78s 171ms/step - loss: 5.0841\n","Epoch 3/30\n","458/458 [==============================] - 78s 170ms/step - loss: 4.5510\n","Epoch 4/30\n","458/458 [==============================] - 78s 171ms/step - loss: 4.0522\n","Epoch 5/30\n","458/458 [==============================] - 78s 171ms/step - loss: 3.5744\n","Epoch 6/30\n","458/458 [==============================] - 79s 173ms/step - loss: 3.1233\n","Epoch 7/30\n","458/458 [==============================] - 78s 171ms/step - loss: 2.7110\n","Epoch 8/30\n","458/458 [==============================] - 79s 172ms/step - loss: 2.3503\n","Epoch 9/30\n","458/458 [==============================] - 78s 171ms/step - loss: 2.0356\n","Epoch 10/30\n","458/458 [==============================] - 79s 173ms/step - loss: 1.7645\n","Epoch 11/30\n","458/458 [==============================] - 78s 171ms/step - loss: 1.5323\n","Epoch 12/30\n","458/458 [==============================] - 79s 173ms/step - loss: 1.3319\n","Epoch 13/30\n","458/458 [==============================] - 78s 171ms/step - loss: 1.1500\n","Epoch 14/30\n","458/458 [==============================] - 79s 172ms/step - loss: 0.9939\n","Epoch 15/30\n","458/458 [==============================] - 78s 171ms/step - loss: 0.8346\n","Epoch 16/30\n","458/458 [==============================] - 79s 172ms/step - loss: 0.7036\n","Epoch 17/30\n","458/458 [==============================] - 79s 171ms/step - loss: 0.5932\n","Epoch 18/30\n","458/458 [==============================] - 79s 172ms/step - loss: 0.4908\n","Epoch 19/30\n","458/458 [==============================] - 78s 171ms/step - loss: 0.4060\n","Epoch 20/30\n","458/458 [==============================] - 79s 172ms/step - loss: 0.3243\n","Epoch 21/30\n","458/458 [==============================] - 78s 170ms/step - loss: 0.2585\n","Epoch 22/30\n","458/458 [==============================] - 79s 173ms/step - loss: 0.1997\n","Epoch 23/30\n","458/458 [==============================] - 78s 171ms/step - loss: 0.1514\n","Epoch 24/30\n","458/458 [==============================] - 78s 171ms/step - loss: 0.1120\n","Epoch 25/30\n","458/458 [==============================] - 78s 170ms/step - loss: 0.0812\n","Epoch 26/30\n","458/458 [==============================] - 78s 171ms/step - loss: 0.0574\n","Epoch 27/30\n","458/458 [==============================] - 78s 170ms/step - loss: 0.0410\n","Epoch 28/30\n","458/458 [==============================] - 79s 172ms/step - loss: 0.0299\n","Epoch 29/30\n","458/458 [==============================] - 78s 171ms/step - loss: 0.0225\n","Epoch 30/30\n","458/458 [==============================] - 79s 172ms/step - loss: 0.0178\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D6fkK0a4d7nE","colab_type":"text"},"source":["Saving the model"]},{"cell_type":"code","metadata":{"id":"aqFgeNxtquvh","colab_type":"code","colab":{}},"source":["textgen.save(\"Goethe.model2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7GVpE_bd_Ba","colab_type":"text"},"source":["Generating the text and downloading it with adjustabel Temperature/Creativity and legth"]},{"cell_type":"markdown","metadata":{"id":"RTa6zf3e_9gV","colab_type":"text"},"source":["[Linktext](https://)You can download a large amount of generated text from your model with the cell below! Rerun the cell as many times as you want for even more text!"]},{"cell_type":"code","metadata":{"id":"-fxL77nvAMAX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1594203367239,"user_tz":-120,"elapsed":23873,"user":{"displayName":"Tilman Fabricius","photoUrl":"","userId":"16612614316801778267"}},"outputId":"ae697400-5ad4-49c5-b084-82ef017ca14c"},"source":["# this temperature schedule cycles between 1 very unexpected token, 1 unexpected token, 2 expected tokens, repeat.\n","# changing the temperature schedule can result in wildly different output!\n","temperature = 0.5  \n","prefix = None  # if you want each generated text to start with a given seed text\n","\n","if train_cfg['line_delimited']:\n","  n = 1000\n","  max_gen_length = 60 if model_cfg['word_level'] else 300\n","else:\n","  n = 1\n","  max_gen_length = 2000 if model_cfg['word_level'] else 10000\n","  \n","timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n","gen_file = '{}_gentext_{}.txt'.format(model_name, timestring)\n","\n","textgen.generate_to_file(gen_file,\n","                         temperature=temperature,\n","                         prefix=prefix,\n","                         n=n,\n","                         max_gen_length=max_gen_length)\n","files.download(gen_file)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_42bd6b9f-8df4-4784-b258-b4691a8ed355\", \"Goethe_all_gentext_20200708_101543.txt\", 8104)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"7s-6P6oNeG7H","colab_type":"text"},"source":["A interactive mode is possible in which the model shows you the n most likely following words and you can pick from them"]},{"cell_type":"code","metadata":{"id":"S1VV94X01R48","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1594201929002,"user_tz":-120,"elapsed":113560,"user":{"displayName":"Tilman Fabricius","photoUrl":"","userId":"16612614316801778267"}},"outputId":"22fc52b5-6752-4282-b74d-9d648aee5b24"},"source":["textgen.generate(interactive=True, top_n=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: \n","\n","\t2: ;\n","\t3: ,\n","\t4: wie\n","\t5: .\n","\n","Progress: \n","\n","Your choice?\n","> 4\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: .\n","\t2: \n","\n","\t3: sei\n","\t4: ,\n","\t5: ?\n","\n","Progress: e\n","\n","Your choice?\n","> 5\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: \n","\n","\t2: ,\n","\t3: «\n","\t4: »ein\n","\t5: male\n","\n","Progress: e ?\n","\n","Your choice?\n","> 4\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: der\n","\t2: ;\n","\t3: .\n","\t4: \n","\n","\t5: ?\n","\n","Progress: e ? »ein\n","\n","Your choice?\n","> 1\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: ,\n","\t2: schnee\n","\t3: \n","\n","\t4: menschen\n","\t5: freunde\n","\n","Progress: e ? »ein der\n","\n","Your choice?\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8f5a57fa6c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, n, return_as_list, prefix, temperature, max_gen_length, interactive, top_n)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                            \u001b[0mmax_gen_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                            \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                            top_n)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/utils.py\u001b[0m in \u001b[0;36mtextgenrnn_generate\u001b[0;34m(model, vocab, indices_char, prefix, temperature, maxlen, meta_token, word_level, single_text, max_gen_length, interactive, top_n)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nProgress: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollapse_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nYour choice?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}